{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construção do Dataset\n",
    "Primeiramente, pretendemos agrupar diversos datasets, utilizando duas colunas: \"text\", texto plano, sendo a segunda coluna a label \"source\" que categoriza o texto como \"human\" ou \"ai\".\n",
    "O segundo passo é extrair as _features_ necessárias para treinar os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed)  # Numpy, é o gerador utilizado pelo sklearn\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # sistema operativo\n",
    "\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montar o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeiro Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jonug\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 1392522/1392522 [00:05<00:00, 237601.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"artem9k/ai-text-detection-pile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0  12 Years a Slave: An Analysis of the Film Essa...  human\n",
       "1  20+ Social Media Post Ideas to Radically Simpl...  human\n",
       "2  2022 Russian Invasion of Ukraine in Global Med...  human\n",
       "3  533 U.S. 27 (2001) Kyllo v. United States: The...  human\n",
       "4  A Charles Schwab Corporation Case Essay\\n\\nCha...  human"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(ds['train'])\n",
    "df_1.drop(columns=['id'], inplace=True)\n",
    "\n",
    "df_1 = df_1.iloc[:, [1,0]]\n",
    "dataframes.append(df_1)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 1000000/1000000 [00:37<00:00, 27021.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds2 = load_dataset(\"dmitva/human_ai_generated_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Also they feel more comfortable at home. Some ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can get another job to work on the weekends,...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>parents and school should agree on the desicio...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Base in my experiences I'm growing, I try hard...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Many people around the world have different ch...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0  Also they feel more comfortable at home. Some ...  human\n",
       "1  I can get another job to work on the weekends,...  human\n",
       "2  parents and school should agree on the desicio...  human\n",
       "3  Base in my experiences I'm growing, I try hard...  human\n",
       "4  Many people around the world have different ch...  human"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2 = pd.DataFrame(ds2['train'])\n",
    "\n",
    "# Create a DataFrame for human text\n",
    "df_human = df_2[['human_text']].copy()\n",
    "df_human = df_human.rename(columns={'human_text': 'text'})\n",
    "df_human['source'] = 'human'\n",
    "\n",
    "# Create a DataFrame for AI text\n",
    "df_ai = df_2[['ai_text']].copy()\n",
    "df_ai = df_ai.rename(columns={'ai_text': 'text'})\n",
    "df_ai['source'] = 'ai'\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "new_df_2 = pd.concat([df_human, df_ai], ignore_index=True)\n",
    "dataframes.append(new_df_2)\n",
    "new_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "human    1000000\n",
      "ai       1000000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(new_df_2['source'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terceiro Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 14000/14000 [00:00<00:00, 52253.38 examples/s]\n",
      "Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 99985.79 examples/s]\n",
      "Generating validation split: 100%|██████████| 3000/3000 [00:00<00:00, 162519.53 examples/s]\n",
      "Generating train split: 100%|██████████| 210000/210000 [00:00<00:00, 341620.08 examples/s]\n",
      "Generating test split: 100%|██████████| 45000/45000 [00:00<00:00, 452801.90 examples/s]\n",
      "Generating validation split: 100%|██████████| 45000/45000 [00:00<00:00, 471568.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds_abs = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"research_abstracts_labeled\")\n",
    "ds_wiki = load_dataset(\"NicolaiSivesind/human-vs-machine\", \"wiki_labeled\")\n",
    "\n",
    "df_3_1_1 = pd.DataFrame(ds_abs['train'])\n",
    "df_3_1_2 = pd.DataFrame(ds_abs['validation'])\n",
    "df_3_1_3 = pd.DataFrame(ds_abs['test'])\n",
    "\n",
    "df_3_2_1 = pd.DataFrame(ds_wiki['train'])\n",
    "df_3_2_2 = pd.DataFrame(ds_wiki['validation'])\n",
    "df_3_2_3 = pd.DataFrame(ds_wiki['test'])\n",
    "\n",
    "df_3 = pd.concat([df_3_1_1, df_3_1_2, df_3_1_3, df_3_2_1, df_3_2_2, df_3_2_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coupling losses were studied in composite tape...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In this study, we investigate the coupling los...</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let $\\mathsf M_{\\mathsf S}$ denote the strong ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In this paper, we investigate Weighted Solyani...</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In 2019 October Betelgeuse began a decline in ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0  Coupling losses were studied in composite tape...  human\n",
       "1  In this study, we investigate the coupling los...     ai\n",
       "2  Let $\\mathsf M_{\\mathsf S}$ denote the strong ...  human\n",
       "3  In this paper, we investigate Weighted Solyani...     ai\n",
       "4  In 2019 October Betelgeuse began a decline in ...  human"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a mapping for the label values\n",
    "label_to_source = {\n",
    "    0: \"human\",\n",
    "    1: \"ai\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to create the \"source\" column\n",
    "df_3['source'] = df_3['label'].map(label_to_source)\n",
    "\n",
    "# Select only the desired columns: \"text\" and \"source\"\n",
    "new_df_3 = df_3[['text', 'source']].copy()\n",
    "\n",
    "dataframes.append(new_df_3)\n",
    "new_df_3.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>y r u always l8 to the meetings?</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The project team embraced a user-centric desig...</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i dont like dealing with risks, it's too stres...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont worry about reliability, it's good enough</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i dont care about human-centered design, just ...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0                   y r u always l8 to the meetings?  human\n",
       "1  The project team embraced a user-centric desig...     ai\n",
       "2  i dont like dealing with risks, it's too stres...  human\n",
       "3   i dont worry about reliability, it's good enough  human\n",
       "4  i dont care about human-centered design, just ...  human"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4 = pd.read_csv(\"LLM.csv\")\n",
    "df_4.rename(columns = {\"Text\": \"text\", \"Label\": \"source\"}, inplace=True)\n",
    "\n",
    "# Create a mapping for the label values\n",
    "label_to_source = {\n",
    "    \"ai\": \"ai\",\n",
    "    \"student\": \"human\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to create the \"source\" column\n",
    "df_4['source'] = df_4['source'].map(label_to_source)\n",
    "df_4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "human    2100\n",
      "ai       1953\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Advanced electromagnetic potentials are indi...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This research paper investigates the question ...</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We give an algorithm for finding network enc...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The paper presents an efficient centralized bi...</td>\n",
       "      <td>ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We introduce an exponential random graph mod...</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0    Advanced electromagnetic potentials are indi...  human\n",
       "1  This research paper investigates the question ...     ai\n",
       "2    We give an algorithm for finding network enc...  human\n",
       "3  The paper presents an efficient centralized bi...     ai\n",
       "4    We introduce an exponential random graph mod...  human"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5 = pd.read_csv(\"data_set.csv\")\n",
    "df_5.rename(columns = {\"abstract\": \"text\", \"is_ai_generated\": \"source\"}, inplace=True)\n",
    "df_5.drop(columns=['title','ai_generated'], inplace=True)\n",
    "\n",
    "# Create a mapping for the label values\n",
    "label_to_source = {\n",
    "    1: \"ai\",\n",
    "    0: \"human\"\n",
    "}\n",
    "\n",
    "# Apply the mapping to create the \"source\" column\n",
    "df_5['source'] = df_5['source'].map(label_to_source)\n",
    "print(df_5['source'].value_counts())\n",
    "dataframes.append(df_5)\n",
    "df_5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sexto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source\n",
      "ai       800\n",
      "human    200\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_6_news_gpt = pd.read_pickle(\"en_news_gpt_features_df.pkl\")\n",
    "df_6_news_human = pd.read_pickle(\"en_news_human_features_df.pkl\")\n",
    "df_6_wiki_gpt = pd.read_pickle(\"en_wiki_gpt_features_df.pkl\")\n",
    "df_6_wiki_human = pd.read_pickle(\"en_wiki_human_features_df.pkl\")\n",
    "\n",
    "df_6_news_gpt = df_6_news_gpt[['text']]\n",
    "df_6_news_gpt['source'] = 'ai'\n",
    "\n",
    "df_6_news_human = df_6_news_human[['text']]\n",
    "df_6_news_human['source'] = 'human'\n",
    "\n",
    "df_6_wiki_gpt = df_6_wiki_gpt[['text']]\n",
    "df_6_wiki_gpt['source'] = 'ai'\n",
    "\n",
    "df_6_wiki_human = df_6_wiki_human[['text']]\n",
    "df_6_wiki_human['source'] = 'human'\n",
    "\n",
    "df_6 = pd.concat([df_6_news_gpt, df_6_news_human, df_6_wiki_gpt, df_6_wiki_human], ignore_index=True)\n",
    "dataframes.append(df_6)\n",
    "print(df_6['source'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntar tudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3717575 entries, 0 to 3717574\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      " 1   source  object\n",
      "dtypes: object(2)\n",
      "memory usage: 56.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3400859 entries, 0 to 3717574\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      " 1   source  object\n",
      "dtypes: object(2)\n",
      "memory usage: 77.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"human_or_ai_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda etapa, extração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 16.0 GiB for an array with shape (2147483648,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize the HashingVectorizer\u001b[39;00m\n\u001b[32m      4\u001b[39m vectorizer = HashingVectorizer(n_features=\u001b[32m5000\u001b[39m, binary=\u001b[38;5;28;01mTrue\u001b[39;00m, lowercase=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create a sparse DataFrame; since HashingVectorizer doesn't store feature names,\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# we generate generic column names.\u001b[39;00m\n\u001b[32m      9\u001b[39m column_names = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfeature_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(X.shape[\u001b[32m1\u001b[39m])]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:878\u001b[39m, in \u001b[36mHashingVectorizer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_ngram_range()\n\u001b[32m    877\u001b[39m analyzer = \u001b[38;5;28mself\u001b[39m.build_analyzer()\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_hasher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m    880\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\feature_extraction\\_hash.py:184\u001b[39m, in \u001b[36mFeatureHasher.transform\u001b[39m\u001b[34m(self, raw_X)\u001b[39m\n\u001b[32m    181\u001b[39m     raw_X_ = chain([first_raw_X], raw_X)\n\u001b[32m    182\u001b[39m     raw_X = (((f, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw_X_)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m indices, indptr, values = \u001b[43m_hashing_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malternate_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m n_samples = indptr.shape[\u001b[32m0\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_samples == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_hashing_fast.pyx:76\u001b[39m, in \u001b[36msklearn.feature_extraction._hashing_fast.transform\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:1612\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(a, new_shape)\u001b[39m\n\u001b[32m   1609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.zeros_like(a, shape=new_shape)\n\u001b[32m   1611\u001b[39m repeats = -(-new_size // a.size)  \u001b[38;5;66;03m# ceil division\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1612\u001b[39m a = \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m)\u001b[49m[:new_size]\n\u001b[32m   1614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reshape(a, new_shape)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 16.0 GiB for an array with shape (2147483648,) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Initialize the HashingVectorizer\n",
    "vectorizer = HashingVectorizer(n_features=5000, binary=True, lowercase=True)\n",
    "X = vectorizer.transform(df['text'])\n",
    "\n",
    "# Create a sparse DataFrame; since HashingVectorizer doesn't store feature names,\n",
    "# we generate generic column names.\n",
    "column_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "df_encoded = pd.DataFrame.sparse.from_spmatrix(X, columns=column_names, dtype=\"int16\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3400859 entries, 0 to 3400858\n",
      "Columns: 1000 entries, 000 to yourself\n",
      "dtypes: int16(1000)\n",
      "memory usage: 6.3 GB\n"
     ]
    }
   ],
   "source": [
    "df_encoded.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(\"human_or_ai_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentar com outras libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"human_or_ai_dataset.csv\")  # Change this to your actual data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3400859 entries, 0 to 3400858\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Dtype \n",
      "---  ------  ----- \n",
      " 0   text    object\n",
      " 1   source  object\n",
      "dtypes: object(2)\n",
      "memory usage: 51.9+ MB\n",
      "None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m vectorizer = HashingVectorizer(n_features=\u001b[32m10000\u001b[39m,binary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Fit and transform the text data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X = \u001b[43mvectorizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1500000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Convert to DataFrame with words as feature names\u001b[39;00m\n\u001b[32m     12\u001b[39m df_encoded = pd.DataFrame(\n\u001b[32m     13\u001b[39m     X.astype(\u001b[33m\"\u001b[39m\u001b[33mint8\u001b[39m\u001b[33m\"\u001b[39m).toarray(), \n\u001b[32m     14\u001b[39m     columns=vectorizer.get_feature_names_out()\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:903\u001b[39m, in \u001b[36mHashingVectorizer.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    885\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    886\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transform a sequence of documents to a document-term matrix.\u001b[39;00m\n\u001b[32m    887\u001b[39m \n\u001b[32m    888\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    901\u001b[39m \u001b[33;03m        Document-term matrix.\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:878\u001b[39m, in \u001b[36mHashingVectorizer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_ngram_range()\n\u001b[32m    877\u001b[39m analyzer = \u001b[38;5;28mself\u001b[39m.build_analyzer()\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_hasher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43manalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m    880\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    321\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    322\u001b[39m         return_tuple = (\n\u001b[32m    323\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    324\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    325\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_hash.py:184\u001b[39m, in \u001b[36mFeatureHasher.transform\u001b[39m\u001b[34m(self, raw_X)\u001b[39m\n\u001b[32m    181\u001b[39m     raw_X_ = chain([first_raw_X], raw_X)\n\u001b[32m    182\u001b[39m     raw_X = (((f, \u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m raw_X_)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m indices, indptr, values = \u001b[43m_hashing_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malternate_sign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    186\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m n_samples = indptr.shape[\u001b[32m0\u001b[39m] - \u001b[32m1\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_samples == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_hashing_fast.pyx:40\u001b[39m, in \u001b[36msklearn.feature_extraction._hashing_fast.transform\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_hash.py:182\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    178\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSamples can not be a single string. The input must be an iterable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    179\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m over iterables of strings.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         )\n\u001b[32m    181\u001b[39m     raw_X_ = chain([first_raw_X], raw_X)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     raw_X = \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_X_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m indices, indptr, values = _hashing_transform(\n\u001b[32m    185\u001b[39m     raw_X, \u001b[38;5;28mself\u001b[39m.n_features, \u001b[38;5;28mself\u001b[39m.dtype, \u001b[38;5;28mself\u001b[39m.alternate_sign, seed=\u001b[32m0\u001b[39m\n\u001b[32m    186\u001b[39m )\n\u001b[32m    187\u001b[39m n_samples = indptr.shape[\u001b[32m0\u001b[39m] - \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:878\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_ngram_range()\n\u001b[32m    877\u001b[39m analyzer = \u001b[38;5;28mself\u001b[39m.build_analyzer()\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m X = \u001b[38;5;28mself\u001b[39m._get_hasher().transform(\u001b[43manalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m X)\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m    880\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:99\u001b[39m, in \u001b[36m_analyze\u001b[39m\u001b[34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Chain together an optional series of text processing steps to go from\u001b[39;00m\n\u001b[32m     78\u001b[39m \u001b[33;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m decoder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     doc = \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m analyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    101\u001b[39m     doc = analyzer(doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonug\\Downloads\\t1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:232\u001b[39m, in \u001b[36m_VectorizerMixin.decode\u001b[39m\u001b[34m(self, doc)\u001b[39m\n\u001b[32m    229\u001b[39m     doc = doc.decode(\u001b[38;5;28mself\u001b[39m.encoding, \u001b[38;5;28mself\u001b[39m.decode_error)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doc \u001b[38;5;129;01mis\u001b[39;00m np.nan:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    233\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnp.nan is an invalid document, expected byte or unicode string.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    234\u001b[39m     )\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "\u001b[31mValueError\u001b[39m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Load large dataset into Dask DataFrame (assuming a CSV file)\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "\n",
    "print(df.info())\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10000,binary=True)\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(df['text'].head(1500000))\n",
    "\n",
    "# Convert to DataFrame with words as feature names\n",
    "df_encoded = pd.DataFrame(\n",
    "    X.astype(\"int8\").toarray(), \n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>...</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youths</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  01  02  03  04  05  06  07  08  ...  youth  youthful  youths  \\\n",
       "0   0    0   0   0   0   0   0   0   0   0  ...      0         0       0   \n",
       "1   0    0   0   0   0   0   0   0   0   0  ...      0         0       0   \n",
       "2   0    0   0   0   0   0   0   0   0   0  ...      0         0       0   \n",
       "3   0    0   0   0   0   0   0   0   0   0  ...      0         0       0   \n",
       "4   0    0   0   0   0   0   0   0   0   0  ...      0         0       0   \n",
       "\n",
       "   youtube  zealand  zero  zombie  zombies  zone  zones  \n",
       "0        0        0     0       0        0     0      0  \n",
       "1        1        0     0       0        0     0      0  \n",
       "2        0        0     0       0        0     0      0  \n",
       "3        0        0     0       0        0     0      0  \n",
       "4        0        0     0       0        0     0      0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
