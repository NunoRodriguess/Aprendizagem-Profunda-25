{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:44:38.529717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5051, 2)\n",
      "Columns: Index(['text', 'source'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 15:44:40.949905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-17 15:44:40.952481: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4040, 600)\n",
      "y_train shape: (4040,)\n",
      "x_test shape: (1011, 600)\n",
      "y_test shape: (1011,)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "csv_path = '../../datasets/human_or_ai_dataset_small.csv'  # Change this to your file path\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Parameters\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].values\n",
    "labels = df['source'].values\n",
    "\n",
    "# Convert labels to numeric values\n",
    "label_map = {'human': 0, 'ai': 1}\n",
    "y_data = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Define TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt to the text dataset\n",
    "text_vectorization.adapt(texts)\n",
    "\n",
    "# Transform text data into tokenized sequences\n",
    "x_data = text_vectorization(texts).numpy()\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Learning\n",
    "Zero-Shot Learning (ZSL) é uma abordagem onde o modelo é capaz de classificar exemplos de classes que nunca viu durante o treino. Para isso, usam-se embeddings pré-treinados (como BERT, GPT, etc.) e é preciso ajustar o modelo para generalizar para novas classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 5s 37ms/step - loss: 0.6654 - accuracy: 0.6010 - val_loss: 0.5733 - val_accuracy: 0.8516\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 4s 33ms/step - loss: 0.3234 - accuracy: 0.9329 - val_loss: 0.1797 - val_accuracy: 0.9535\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 4s 31ms/step - loss: 0.1088 - accuracy: 0.9748 - val_loss: 0.1377 - val_accuracy: 0.9634\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 4s 32ms/step - loss: 0.0575 - accuracy: 0.9894 - val_loss: 0.1185 - val_accuracy: 0.9654\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 4s 32ms/step - loss: 0.0384 - accuracy: 0.9938 - val_loss: 0.1226 - val_accuracy: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x791f40511c00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero-Shot Learning Model\n",
    "def create_zero_shot_model():\n",
    "    inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
    "    x = layers.Embedding(max_tokens, 128)(inputs)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Compile and train Zero-Shot Model\n",
    "zero_shot_model = create_zero_shot_model()\n",
    "zero_shot_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "zero_shot_model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Shot Learning\n",
    "One-Shot Learning (OSL) é uma abordagem onde o modelo é treinado para classificar exemplos de classes com base num único exemplo de cada classe. Isto é útil quando há poucos dados disponíveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "127/127 [==============================] - 4s 33ms/step - loss: 0.6659 - accuracy: 0.6052 - val_loss: 0.5911 - val_accuracy: 0.9327\n",
      "Epoch 2/5\n",
      "127/127 [==============================] - 4s 32ms/step - loss: 0.3577 - accuracy: 0.9540 - val_loss: 0.2011 - val_accuracy: 0.9535\n",
      "Epoch 3/5\n",
      "127/127 [==============================] - 4s 31ms/step - loss: 0.1225 - accuracy: 0.9725 - val_loss: 0.1318 - val_accuracy: 0.9604\n",
      "Epoch 4/5\n",
      "127/127 [==============================] - 4s 32ms/step - loss: 0.0674 - accuracy: 0.9859 - val_loss: 0.1202 - val_accuracy: 0.9614\n",
      "Epoch 5/5\n",
      "127/127 [==============================] - 5s 42ms/step - loss: 0.0423 - accuracy: 0.9936 - val_loss: 0.1215 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x791f36ee2b60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-Shot Learning Model\n",
    "def create_one_shot_model():\n",
    "    inputs = keras.Input(shape=(max_length,), dtype=\"int32\")\n",
    "    x = layers.Embedding(max_tokens, 128)(inputs)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Compile and train One-Shot Model\n",
    "one_shot_model = create_one_shot_model()\n",
    "one_shot_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "one_shot_model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1226 - accuracy: 0.9575\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 0.1215 - accuracy: 0.9565\n",
      "Zero-Shot Model - Loss: 0.12259894609451294 Accuracy: 0.9574678540229797\n",
      "One-Shot Model - Loss: 0.1214541643857956 Accuracy: 0.9564787149429321\n"
     ]
    }
   ],
   "source": [
    "# Evaluate models\n",
    "zero_shot_loss, zero_shot_accuracy = zero_shot_model.evaluate(x_test, y_test)\n",
    "one_shot_loss, one_shot_accuracy = one_shot_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Zero-Shot Model - Loss:\", zero_shot_loss, \"Accuracy:\", zero_shot_accuracy)\n",
    "print(\"One-Shot Model - Loss:\", one_shot_loss, \"Accuracy:\", one_shot_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Bart "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto:   Advanced electromagnetic potentials are indigenous to the classical Maxwell\n",
      "theory. Generally however they are deemed undesirable and are forcibly\n",
      "e...\n",
      "Classificação: ai (Confiança: 0.6850)\n",
      "\n",
      "Texto: This research paper investigates the question of whether advanced potentials are anomalous. Advanced potentials are a type of psychic phenomenon that ...\n",
      "Classificação: ai (Confiança: 0.6683)\n",
      "\n",
      "Texto:   We give an algorithm for finding network encoding and decoding equations for\n",
      "error-free multicasting networks with multiple sources and sinks. The a...\n",
      "Classificação: ai (Confiança: 0.7466)\n",
      "\n",
      "Texto: The paper presents an efficient centralized binary multicast network coding algorithm for any cyclic network. The proposed algorithm aims to optimize ...\n",
      "Classificação: ai (Confiança: 0.7806)\n",
      "\n",
      "Texto:   We introduce an exponential random graph model for networks with a fixed\n",
      "degree distribution and with a tunable degree-degree correlation. We then\n",
      "i...\n",
      "Classificação: ai (Confiança: 0.7968)\n",
      "\n",
      "Texto: This research paper investigates the percolation transition in networks with degree-degree correlation, which refers to the relationship between the d...\n",
      "Classificação: ai (Confiança: 0.6867)\n",
      "\n",
      "Texto:   We obtain new invariants of topological link concordance and homology\n",
      "cobordism of 3-manifolds from Hirzebruch-type intersection form defects of\n",
      "tow...\n",
      "Classificação: ai (Confiança: 0.8144)\n",
      "\n",
      "Texto: This research paper delves into the area of algebraic topology, where we investigate the relationship between link concordance, homology cobordism, an...\n",
      "Classificação: ai (Confiança: 0.7871)\n",
      "\n",
      "Texto:   The identification of the limiting factors in the dynamical behavior of\n",
      "complex systems is an important interdisciplinary problem which often can be...\n",
      "Classificação: ai (Confiança: 0.8064)\n",
      "\n",
      "Texto: This research paper investigates the potential use of bounding network spectra as a basis for network design. The paper explores the concept of networ...\n",
      "Classificação: ai (Confiança: 0.7098)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Extrair textos e labels\n",
    "texts = df['text'].values[:10]  # Selecionar apenas as primeiras 10 entradas para o teste\n",
    "labels = df['source'].values[:10]  # Labels correspondentes\n",
    "\n",
    "# Converter labels para valores numéricos\n",
    "label_map = {'human': 0, 'ai': 1}\n",
    "y_data = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Carregar o pipeline para classificação Zero-Shot com BERT\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Função para classificar com BERT\n",
    "def classify_with_bart(texts, candidate_labels):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        result = zero_shot_classifier(text, candidate_labels)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# Lista de rótulos candidatos\n",
    "candidate_labels = ['human', 'ai']\n",
    "\n",
    "# Classificar os textos do dataset com o modelo Zero-Shot BERT\n",
    "classification_results = classify_with_bart(texts, candidate_labels)\n",
    "\n",
    "# Mostrar as classificações\n",
    "for i, (text, result) in enumerate(zip(texts, classification_results)):\n",
    "    print(f\"Texto: {text[:150]}...\")  # Exibir apenas os primeiros 150 caracteres do texto\n",
    "    print(f\"Classificação: {result['labels'][0]} (Confiança: {result['scores'][0]:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "O Prompt Engineering é a prática de projetar prompts (entradas) de forma que o modelo gere saídas de alta qualidade. A ideia é fornecer ao modelo o contexto e as instruções mais claras e específicas para obter a resposta desejada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG (Retrieval-Augmented Generation)\n",
    "O RAG é uma técnica que combina recuperação de informações com geração de texto. Em vez de confiar apenas na capacidade do modelo de gerar respostas com base no seu treino, o modelo também consulta fontes externas de dados (como uma base de dados ou documentos relevantes) para melhorar a qualidade e a precisão das respostas geradas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting for the Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Prediction dataset shape: (30, 2)\n",
      "Columns: Index(['ID', 'Text'], dtype='object')\n",
      "Sample IDs: 0    D1-1\n",
      "1    D1-2\n",
      "2    D1-3\n",
      "3    D1-4\n",
      "4    D1-5\n",
      "Name: ID, dtype: object\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 177ms/step\n",
      "\n",
      "Amostra dos resultados de previsão:\n",
      "     ID  Label\n",
      "0  D1-1  Human\n",
      "1  D1-2  Human\n",
      "2  D1-3  Human\n",
      "3  D1-4  Human\n",
      "4  D1-5     AI\n",
      "\n",
      "Resultados salvos em prediction_results3.csv\n"
     ]
    }
   ],
   "source": [
    "############ NAO IMPLEMENTEI ISTO\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model('best_model_rnn_gru.h5')\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Carregar o CSV com dados para prever\n",
    "prediction_csv_path = '../../datasets/dataset1_inputs.csv'\n",
    "df_predict = pd.read_csv(prediction_csv_path,sep=\"\\t\")\n",
    "\n",
    "# Verificar os dados carregados\n",
    "print(\"Prediction dataset shape:\", df_predict.shape)\n",
    "print(\"Columns:\", df_predict.columns)\n",
    "print(\"Sample IDs:\", df_predict['ID'].head())\n",
    "\n",
    "# Pré-processar os dados de texto para corresponder ao formato de treinamento\n",
    "# Converter textos para sequências\n",
    "sequences = tokenizer.texts_to_sequences(df_predict['Text'].values)\n",
    "\n",
    "# Padding das sequências para o mesmo tamanho usado no treinamento\n",
    "x_predict = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Fazer previsões com o modelo treinado\n",
    "predictions = model.predict(x_predict)\n",
    "\n",
    "# Converter probabilidades para labels binários (0 = Human, 1 = AI)\n",
    "# Usando 0.5 como threshold - você pode ajustar isso conforme necessário\n",
    "labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Mapear labels para \"AI\" e \"Human\"\n",
    "label_mapping = {1: \"AI\", 0: \"Human\"}\n",
    "labels_mapped = [label_mapping[label] for label in labels.flatten()]\n",
    "\n",
    "# Criar um DataFrame com os resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': df_predict['ID'],\n",
    "    'Label': labels_mapped\n",
    "})\n",
    "\n",
    "# Exibir uma amostra dos resultados\n",
    "print(\"\\nAmostra dos resultados de previsão:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Salvar em CSV\n",
    "output_csv_path = 'prediction_results3.csv'\n",
    "results_df.to_csv(output_csv_path, sep=\"\\t\", index=False)\n",
    "print(f\"\\nResultados salvos em {output_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
