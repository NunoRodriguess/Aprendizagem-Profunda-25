{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed) # Python\n",
    "    np.random.seed(seed)  # Numpy, Ã© o gerador utilizado pelo sklearn\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # sistema operativo\n",
    "\n",
    "set_seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5051, 2)\n",
      "Columns: Index(['text', 'source'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Dropout, Input, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load dos dados\n",
    "csv_path = '../../datasets/human_or_ai_dataset_small.csv'  # Change this to your file path\n",
    "df = pd.read_csv(csv_path)\n",
    "# Sanity check!\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (4040, 600)\n",
      "y_train shape: (4040,)\n",
      "x_test shape: (1011, 600)\n",
      "y_test shape: (1011,)\n",
      "Sample sequence: [   15    20     2   680     3  4751   121  6079  3152     6     2   624\n",
      "  1302   524     6     5   216   432     3     1  1533     1  3011   104\n",
      "    15   954     2  2998   505 18556    23  5046  6078     6    38   104\n",
      "     4    56     9   121  6078     8   224    10     2   583   144  8081\n",
      "  3011    44    15   129     9 18556  7232    91     2   627   378     8\n",
      "   150  3530    15    56     9   121  6078     8    31   224     6     5\n",
      "  6757    44    11    24   489   463     7   246    23  1891     6   627\n",
      "    47    67     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "Sample label: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df['text'].values\n",
    "labels = df['source'].values\n",
    "\n",
    "# Convert labels to numeric values\n",
    "label_map = {'human': 0, 'ai': 1}\n",
    "y_data = np.array([label_map[label] for label in labels])\n",
    "\n",
    "# Define TextVectorization layer\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "# Adapt to the text dataset\n",
    "text_vectorization.adapt(texts)\n",
    "\n",
    "# Transform text data into tokenized sequences\n",
    "x_data = text_vectorization(texts).numpy()  # Convert TensorFlow tensor to NumPy array\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "# Print a sample\n",
    "print(\"Sample sequence:\", x_train[0])\n",
    "print(\"Sample label:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 16\n",
    "val_fraction = 0.15\n",
    "seed = 25\n",
    "\n",
    "# Shuffle the entire dataset with a fixed seed\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=len(x_train), seed=seed)\n",
    "\n",
    "# Calculate split sizes\n",
    "val_size = int(len(x_train) * val_fraction)\n",
    "train_size = len(x_train) - val_size\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_ds = dataset.skip(val_size).batch(batch_size)\n",
    "val_ds = dataset.take(val_size).batch(batch_size)\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "train_ds = dataset.skip(val_size).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforme com Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "#    def compute_mask(self, inputs, mask=None):\n",
    "#        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\"> Layer (type)                    </span>â<span style=\"font-weight: bold\"> Output Shape           </span>â<span style=\"font-weight: bold\">       Param # </span>â\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â positional_embedding_4          â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,273,600</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)           â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â transformer_encoder_4           â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      â       <span style=\"color: #00af00; text-decoration-color: #00af00\">543,776</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â global_max_pooling1d_4          â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "â (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           â             \u001b[38;5;34m0\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â positional_embedding_4          â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â     \u001b[38;5;34m5,273,600\u001b[0m â\n",
       "â (\u001b[38;5;33mPositionalEmbedding\u001b[0m)           â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â transformer_encoder_4           â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      â       \u001b[38;5;34m543,776\u001b[0m â\n",
       "â (\u001b[38;5;33mTransformerEncoder\u001b[0m)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â global_max_pooling1d_4          â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â             \u001b[38;5;34m0\u001b[0m â\n",
       "â (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â                        â               â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â             \u001b[38;5;34m0\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ¼âââââââââââââââââââââââââ¼ââââââââââââââââ¤\n",
       "â dense_14 (\u001b[38;5;33mDense\u001b[0m)                â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â           \u001b[38;5;34m257\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,817,633</span> (22.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,817,633\u001b[0m (22.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 383ms/step - accuracy: 0.6576 - loss: 0.8759"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 409ms/step - accuracy: 0.6581 - loss: 0.8742 - val_accuracy: 0.8960 - val_loss: 0.2026\n",
      "Epoch 2/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step - accuracy: 0.8859 - loss: 0.2757"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 425ms/step - accuracy: 0.8859 - loss: 0.2756 - val_accuracy: 0.9670 - val_loss: 0.0989\n",
      "Epoch 3/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 430ms/step - accuracy: 0.9386 - loss: 0.1595 - val_accuracy: 0.9422 - val_loss: 0.1242\n",
      "Epoch 4/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 409ms/step - accuracy: 0.9560 - loss: 0.1147"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 435ms/step - accuracy: 0.9560 - loss: 0.1147 - val_accuracy: 0.9719 - val_loss: 0.0847\n",
      "Epoch 5/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9566 - loss: 0.1061"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 446ms/step - accuracy: 0.9566 - loss: 0.1061 - val_accuracy: 0.9818 - val_loss: 0.0558\n",
      "Epoch 6/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 453ms/step - accuracy: 0.9644 - loss: 0.0877 - val_accuracy: 0.9736 - val_loss: 0.0873\n",
      "Epoch 7/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 454ms/step - accuracy: 0.9762 - loss: 0.0747 - val_accuracy: 0.9109 - val_loss: 0.2010\n",
      "Epoch 8/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step - accuracy: 0.9721 - loss: 0.0766"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 448ms/step - accuracy: 0.9722 - loss: 0.0766 - val_accuracy: 0.9901 - val_loss: 0.0270\n",
      "Epoch 9/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 447ms/step - accuracy: 0.9858 - loss: 0.0369 - val_accuracy: 0.9901 - val_loss: 0.0360\n",
      "Epoch 10/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 426ms/step - accuracy: 0.9886 - loss: 0.0361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 452ms/step - accuracy: 0.9886 - loss: 0.0361 - val_accuracy: 0.9934 - val_loss: 0.0179\n",
      "Epoch 11/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 420ms/step - accuracy: 0.9871 - loss: 0.0297"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 446ms/step - accuracy: 0.9871 - loss: 0.0297 - val_accuracy: 1.0000 - val_loss: 0.0042\n",
      "Epoch 12/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 449ms/step - accuracy: 0.9890 - loss: 0.0353 - val_accuracy: 1.0000 - val_loss: 0.0048\n",
      "Epoch 13/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 450ms/step - accuracy: 0.9926 - loss: 0.0215 - val_accuracy: 0.9917 - val_loss: 0.0284\n",
      "Epoch 14/20\n",
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 428ms/step - accuracy: 0.9970 - loss: 0.0128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 454ms/step - accuracy: 0.9969 - loss: 0.0128 - val_accuracy: 1.0000 - val_loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'positional_embedding_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'transformer_encoder_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 151ms/step - accuracy: 0.9437 - loss: 0.1347\n",
      "Test acc: 0.949\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "# Early stopping \n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.h5\",\n",
    "                                    save_best_only=True),\n",
    "                                   early_stopping\n",
    "]\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder.keras\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - accuracy: 0.9437 - loss: 0.1347\n",
      "Test acc: 0.949\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test acc: {model.evaluate(test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
